{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b360b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "from src import models, data, lens, functional\n",
    "from src.utils import experiment_utils\n",
    "from baukit import Menu, show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7d893b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated: 11.82 GB\n",
      "Reserved: 12.74 GB\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "from src import models, data, lens, functional\n",
    "from src.utils import experiment_utils\n",
    "from baukit import Menu, show\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "print(f\"Allocated: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n",
    "print(f\"Reserved: {torch.cuda.memory_reserved(0) / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9d46d0c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at EleutherAI/gpt-j-6B were not used when initializing GPTJForCausalLM: ['transformer.h.0.attn.bias', 'transformer.h.0.attn.masked_bias', 'transformer.h.1.attn.bias', 'transformer.h.1.attn.masked_bias', 'transformer.h.10.attn.bias', 'transformer.h.10.attn.masked_bias', 'transformer.h.11.attn.bias', 'transformer.h.11.attn.masked_bias', 'transformer.h.12.attn.bias', 'transformer.h.12.attn.masked_bias', 'transformer.h.13.attn.bias', 'transformer.h.13.attn.masked_bias', 'transformer.h.14.attn.bias', 'transformer.h.14.attn.masked_bias', 'transformer.h.15.attn.bias', 'transformer.h.15.attn.masked_bias', 'transformer.h.16.attn.bias', 'transformer.h.16.attn.masked_bias', 'transformer.h.17.attn.bias', 'transformer.h.17.attn.masked_bias', 'transformer.h.18.attn.bias', 'transformer.h.18.attn.masked_bias', 'transformer.h.19.attn.bias', 'transformer.h.19.attn.masked_bias', 'transformer.h.2.attn.bias', 'transformer.h.2.attn.masked_bias', 'transformer.h.20.attn.bias', 'transformer.h.20.attn.masked_bias', 'transformer.h.21.attn.bias', 'transformer.h.21.attn.masked_bias', 'transformer.h.22.attn.bias', 'transformer.h.22.attn.masked_bias', 'transformer.h.23.attn.bias', 'transformer.h.23.attn.masked_bias', 'transformer.h.24.attn.bias', 'transformer.h.24.attn.masked_bias', 'transformer.h.25.attn.bias', 'transformer.h.25.attn.masked_bias', 'transformer.h.26.attn.bias', 'transformer.h.26.attn.masked_bias', 'transformer.h.27.attn.bias', 'transformer.h.27.attn.masked_bias', 'transformer.h.3.attn.bias', 'transformer.h.3.attn.masked_bias', 'transformer.h.4.attn.bias', 'transformer.h.4.attn.masked_bias', 'transformer.h.5.attn.bias', 'transformer.h.5.attn.masked_bias', 'transformer.h.6.attn.bias', 'transformer.h.6.attn.masked_bias', 'transformer.h.7.attn.bias', 'transformer.h.7.attn.masked_bias', 'transformer.h.8.attn.bias', 'transformer.h.8.attn.masked_bias', 'transformer.h.9.attn.bias', 'transformer.h.9.attn.masked_bias']\n",
      "- This IS expected if you are initializing GPTJForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPTJForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dtype: torch.float16, device: cuda:0, memory: 12101765568\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda:0\"\n",
    "mt = models.load_model(\"gptj\", device=device, fp16=True)\n",
    "print(f\"dtype: {mt.model.dtype}, device: {mt.model.device}, memory: {mt.model.get_memory_footprint()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "5f51154e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available relations: 47\n",
      "Selected: characteristic gender\n"
     ]
    }
   ],
   "source": [
    "dataset = data.load_dataset()\n",
    "\n",
    "relation_names = [r.name for r in dataset.relations]\n",
    "\n",
    "# Manual selection instead of baukit.Menu (not supported in VS Code)\n",
    "# Choose a relation by uncommenting one of the lines below or setting relation_name directly\n",
    "relation_name = relation_names[0]  # Default to first relation\n",
    "# relation_name = \"person_occupation\"  # Or specify a relation name directly\n",
    "\n",
    "print(f\"Available relations: {len(relation_names)}\")\n",
    "print(f\"Selected: {relation_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "3a17444f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "characteristic gender -- 30 samples\n",
      "------------------------------------------------------\n",
      "persuasiveness -> men\n",
      "multitasking -> women\n",
      "logical thinking -> men\n",
      "nurturing -> women\n"
     ]
    }
   ],
   "source": [
    "# relation_name is set in the previous cell\n",
    "relation = dataset.filter(relation_names=[relation_name])[0]\n",
    "print(f\"{relation.name} -- {len(relation.samples)} samples\")\n",
    "print(\"------------------------------------------------------\")\n",
    "\n",
    "experiment_utils.set_seed(12345) # set seed to a constant value for sampling consistency\n",
    "train, test = relation.split(4)\n",
    "print(\"\\n\".join([sample.__str__() for sample in train.samples]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "145bf9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "################### hparams ###################\n",
    "layer = 5\n",
    "beta = 2.5\n",
    "###############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d642fd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable gradient checkpointing to save memory can remove later not sure if relevant\n",
    "#mt.model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "83b33032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepended text (not 'trained' on): The following 4 sentences are accurate.\n",
      "\n",
      "Jacobian computed over 4 samples:\n",
      "  1. persuasiveness -> men\n",
      "  2. multitasking -> women\n",
      "  3. logical thinking -> men\n",
      "  4. nurturing -> women\n",
      "\n",
      "✓ Operator created successfully\n",
      "  Structure: Custom prepended text + 4 for Jacobian (leave-one-out)\n",
      "\n",
      "Prompt structure during training:\n",
      "  The following 4 sentences are accurate.  [Prepended: in Jacobian]\n",
      "  persuasiveness -> men  [Sample 1: leave-one-out]\n",
      "  multitasking -> women  [Sample 2: leave-one-out]\n",
      "  logical thinking -> men  [Sample 3: leave-one-out]\n",
      "  nurturing -> women  [Sample 4: leave-one-out]\n",
      "  [Query subject]\n",
      "\n",
      "✓ Operator created successfully\n",
      "  Structure: Custom prepended text + 4 for Jacobian (leave-one-out)\n",
      "\n",
      "Prompt structure during training:\n",
      "  The following 4 sentences are accurate.  [Prepended: in Jacobian]\n",
      "  persuasiveness -> men  [Sample 1: leave-one-out]\n",
      "  multitasking -> women  [Sample 2: leave-one-out]\n",
      "  logical thinking -> men  [Sample 3: leave-one-out]\n",
      "  nurturing -> women  [Sample 4: leave-one-out]\n",
      "  [Query subject]\n"
     ]
    }
   ],
   "source": [
    "# Clear memory before creating operator\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# Reload the modules to get the latest changes\n",
    "import importlib\n",
    "import src.operators\n",
    "import src.functional\n",
    "importlib.reload(src.functional)\n",
    "importlib.reload(src.operators)\n",
    "\n",
    "from src.operators import JacobianIclMeanEstimator\n",
    "\n",
    "# Use a custom string to prepend\n",
    "#prepend_text = \"The following 4 sentences are completely wrong.\\n\"\n",
    "prepend_text = \"The following 4 sentences are accurate.\\n\"\n",
    "\n",
    "jacobian_samples = train.samples\n",
    "\n",
    "print(f\"Prepended text (not 'trained' on): {prepend_text.strip()}\")\n",
    "print(f\"\\nJacobian computed over {len(jacobian_samples)} samples:\")\n",
    "for i, s in enumerate(jacobian_samples, 1):\n",
    "    print(f\"  {i}. {s}\")\n",
    "\n",
    "estimator = JacobianIclMeanEstimator(\n",
    "    mt = mt, \n",
    "    h_layer = layer,\n",
    "    beta = beta,\n",
    "    prepend_string=prepend_text,\n",
    ")\n",
    "operator = estimator(\n",
    "    relation.set(\n",
    "        samples=jacobian_samples,  # Jacobian computed over these (leave-one-out)\n",
    "    )\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Operator created successfully\")\n",
    "print(f\"  Structure: Custom prepended text + {len(jacobian_samples)} for Jacobian (leave-one-out)\")\n",
    "print(f\"\\nPrompt structure during training:\")\n",
    "print(f\"  {prepend_text.strip()}  [Prepended: in Jacobian]\")\n",
    "for i, s in enumerate(jacobian_samples, 1):\n",
    "    print(f\"  {s.subject} -> {s.object}  [Sample {i}: leave-one-out]\")\n",
    "print(f\"  [Query subject]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1b6eda",
   "metadata": {},
   "source": [
    "# Checking $faithfulness$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6517ce0",
   "metadata": {},
   "source": [
    "test runs the LLM on the test data to filter out the ones where it succeeds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "9d79b613",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = functional.filter_relation_samples_based_on_provided_fewshots(\n",
    "    mt=mt, test_relation=test, prompt_template=operator.prompt_template, batch_size=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "9e02634b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adventurousness -> men\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PredictedToken(token=' women', prob=0.7289615869522095),\n",
       " PredictedToken(token=' men', prob=0.24801670014858246),\n",
       " PredictedToken(token=' girls', prob=0.004989052657037973),\n",
       " PredictedToken(token=' both', prob=0.0016324118478223681),\n",
       " PredictedToken(token=' females', prob=0.0014979852130636573)]"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = test.samples[0]\n",
    "print(sample)\n",
    "operator(subject = sample.subject).predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "e717d47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hs_and_zs = functional.compute_hs_and_zs(\n",
    "    mt = mt,\n",
    "    prompt_template = operator.prompt_template,\n",
    "    subjects = [sample.subject],\n",
    "    h_layer= operator.h_layer,\n",
    ")\n",
    "h = hs_and_zs.h_by_subj[sample.subject]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c69136b",
   "metadata": {},
   "source": [
    "## Approximating LM computation $F$ as an affine transformation\n",
    "\n",
    "### $$ F(\\mathbf{s}, c_r) \\approx \\beta \\, W_r \\mathbf{s} + b_r $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "d8bf8b1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([(' women', 0.729),\n",
       "  (' men', 0.248),\n",
       "  (' girls', 0.005),\n",
       "  (' both', 0.002),\n",
       "  (' females', 0.001),\n",
       "  (' boys', 0.001),\n",
       "  (' femin', 0.001),\n",
       "  (' creativity', 0.001),\n",
       "  (' the', 0.001),\n",
       "  (' intro', 0.001)],\n",
       " {})"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = operator.beta * (operator.weight @ h) + operator.bias\n",
    "\n",
    "lens.logit_lens(\n",
    "    mt = mt,\n",
    "    h = z,\n",
    "    get_proba = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "2cb7d4e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[RelationSample(subject='adventurousness', object='men'),\n",
       " RelationSample(subject='aggressiveness', object='men'),\n",
       " RelationSample(subject='ambition', object='men'),\n",
       " RelationSample(subject='assertiveness', object='men'),\n",
       " RelationSample(subject='bravery', object='men'),\n",
       " RelationSample(subject='compassion', object='women'),\n",
       " RelationSample(subject='competitiveness', object='men'),\n",
       " RelationSample(subject='confidence', object='men'),\n",
       " RelationSample(subject='critical thinking', object='men'),\n",
       " RelationSample(subject='decisiveness', object='men'),\n",
       " RelationSample(subject='discipline', object='men'),\n",
       " RelationSample(subject='empathy', object='women'),\n",
       " RelationSample(subject='endurance', object='men'),\n",
       " RelationSample(subject='independence', object='men'),\n",
       " RelationSample(subject='intuition', object='women'),\n",
       " RelationSample(subject='leadership', object='men'),\n",
       " RelationSample(subject='resilience', object='men'),\n",
       " RelationSample(subject='risk-taking', object='men'),\n",
       " RelationSample(subject='sensitivity', object='women')]"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "e0725d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample.subject='adventurousness', sample.object='men', predicted=\" women\", (p=0.7289615869522095), known=(✗)\n",
      "sample.subject='aggressiveness', sample.object='men', predicted=\" women\", (p=0.663330614566803), known=(✗)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample.subject='ambition', sample.object='men', predicted=\" men\", (p=0.5623860359191895), known=(✓)\n",
      "sample.subject='assertiveness', sample.object='men', predicted=\" women\", (p=0.8608171343803406), known=(✗)\n",
      "sample.subject='bravery', sample.object='men', predicted=\" women\", (p=0.5630571842193604), known=(✗)\n",
      "sample.subject='compassion', sample.object='women', predicted=\" women\", (p=0.8373768925666809), known=(✓)\n",
      "sample.subject='competitiveness', sample.object='men', predicted=\" women\", (p=0.5813226103782654), known=(✗)\n",
      "sample.subject='compassion', sample.object='women', predicted=\" women\", (p=0.8373768925666809), known=(✓)\n",
      "sample.subject='competitiveness', sample.object='men', predicted=\" women\", (p=0.5813226103782654), known=(✗)\n",
      "sample.subject='confidence', sample.object='men', predicted=\" women\", (p=0.539031982421875), known=(✗)\n",
      "sample.subject='critical thinking', sample.object='men', predicted=\" women\", (p=0.623507022857666), known=(✗)\n",
      "sample.subject='decisiveness', sample.object='men', predicted=\" women\", (p=0.7270858883857727), known=(✗)\n",
      "sample.subject='confidence', sample.object='men', predicted=\" women\", (p=0.539031982421875), known=(✗)\n",
      "sample.subject='critical thinking', sample.object='men', predicted=\" women\", (p=0.623507022857666), known=(✗)\n",
      "sample.subject='decisiveness', sample.object='men', predicted=\" women\", (p=0.7270858883857727), known=(✗)\n",
      "sample.subject='discipline', sample.object='men', predicted=\" women\", (p=0.5264520645141602), known=(✗)\n",
      "sample.subject='empathy', sample.object='women', predicted=\" women\", (p=0.6269864439964294), known=(✓)\n",
      "sample.subject='discipline', sample.object='men', predicted=\" women\", (p=0.5264520645141602), known=(✗)\n",
      "sample.subject='empathy', sample.object='women', predicted=\" women\", (p=0.6269864439964294), known=(✓)\n",
      "sample.subject='endurance', sample.object='men', predicted=\" women\", (p=0.5809364318847656), known=(✗)\n",
      "sample.subject='independence', sample.object='men', predicted=\" women\", (p=0.4609493911266327), known=(✗)\n",
      "sample.subject='intuition', sample.object='women', predicted=\" women\", (p=0.8582093119621277), known=(✓)\n",
      "sample.subject='endurance', sample.object='men', predicted=\" women\", (p=0.5809364318847656), known=(✗)\n",
      "sample.subject='independence', sample.object='men', predicted=\" women\", (p=0.4609493911266327), known=(✗)\n",
      "sample.subject='intuition', sample.object='women', predicted=\" women\", (p=0.8582093119621277), known=(✓)\n",
      "sample.subject='leadership', sample.object='men', predicted=\" leadership\", (p=0.9089637994766235), known=(✗)\n",
      "sample.subject='resilience', sample.object='men', predicted=\" women\", (p=0.6113225221633911), known=(✗)\n",
      "sample.subject='leadership', sample.object='men', predicted=\" leadership\", (p=0.9089637994766235), known=(✗)\n",
      "sample.subject='resilience', sample.object='men', predicted=\" women\", (p=0.6113225221633911), known=(✗)\n",
      "sample.subject='risk-taking', sample.object='men', predicted=\" women\", (p=0.6828121542930603), known=(✗)\n",
      "sample.subject='sensitivity', sample.object='women', predicted=\" women\", (p=0.49700379371643066), known=(✓)\n",
      "------------------------------------------------------------\n",
      "Faithfulness (@1) = 0.2631578947368421\n",
      "------------------------------------------------------------\n",
      "sample.subject='risk-taking', sample.object='men', predicted=\" women\", (p=0.6828121542930603), known=(✗)\n",
      "sample.subject='sensitivity', sample.object='women', predicted=\" women\", (p=0.49700379371643066), known=(✓)\n",
      "------------------------------------------------------------\n",
      "Faithfulness (@1) = 0.2631578947368421\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "wrong = 0\n",
    "for sample in test.samples:\n",
    "    predictions = operator(subject = sample.subject).predictions\n",
    "    known_flag = functional.is_nontrivial_prefix(\n",
    "        prediction=predictions[0].token, target=sample.object\n",
    "    )\n",
    "    print(f\"{sample.subject=}, {sample.object=}, \", end=\"\")\n",
    "    print(f'predicted=\"{functional.format_whitespace(predictions[0].token)}\", (p={predictions[0].prob}), known=({functional.get_tick_marker(known_flag)})')\n",
    "    \n",
    "    correct += known_flag\n",
    "    wrong += not known_flag\n",
    "    \n",
    "faithfulness = correct/(correct + wrong)\n",
    "\n",
    "print(\"------------------------------------------------------------\")\n",
    "print(f\"Faithfulness (@1) = {faithfulness}\")\n",
    "print(\"------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "74c61f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PROMPT TEMPLATE STORED IN OPERATOR:\n",
      "================================================================================\n",
      "'<|endoftext|>persuasiveness is commonly associated with men\\nmultitasking is commonly associated with women\\nlogical thinking is commonly associated with men\\nnurturing is commonly associated with women\\n{} is commonly associated with'\n",
      "\n",
      "================================================================================\n",
      "EXAMPLE PROMPT FOR FIRST TEST SAMPLE:\n",
      "================================================================================\n",
      "<|endoftext|>persuasiveness is commonly associated with men\n",
      "multitasking is commonly associated with women\n",
      "logical thinking is commonly associated with men\n",
      "nurturing is commonly associated with women\n",
      "adventurousness is commonly associated with\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Let's see what the actual prompt looks like\n",
    "print(\"=\" * 80)\n",
    "print(\"PROMPT TEMPLATE STORED IN OPERATOR:\")\n",
    "print(\"=\" * 80)\n",
    "print(repr(operator.prompt_template))\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EXAMPLE PROMPT FOR FIRST TEST SAMPLE:\")\n",
    "print(\"=\" * 80)\n",
    "example_prompt = operator.prompt_template.format(test.samples[0].subject)\n",
    "print(example_prompt)\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a13389",
   "metadata": {},
   "source": [
    "# $causality$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "da2f8eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "################### hparams ###################\n",
    "rank = 100\n",
    "###############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "25ac7213",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_utils.set_seed(12345) # set seed to a constant value for sampling consistency\n",
    "test_targets = functional.random_edit_targets(test.samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d83c9b",
   "metadata": {},
   "source": [
    "## setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "1a13c0ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Changing the mapping (adventurousness -> men) to (adventurousness -> women)'"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source = test.samples[0]\n",
    "target = test_targets[source]\n",
    "\n",
    "f\"Changing the mapping ({source}) to ({source.subject} -> {target.object})\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98f67c8",
   "metadata": {},
   "source": [
    "### Calculate $\\Delta \\mathbf{s}$ such that $\\mathbf{s} + \\Delta \\mathbf{s} \\approx \\mathbf{s}'$\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img align=\"center\" src=\"causality-crop.png\" style=\"width:80%;\"/>\n",
    "</p>\n",
    "\n",
    "Under the relation $r =\\, $*plays the instrument*, and given the subject $s =\\, $*Miles Davis*, the model will predict $o =\\, $*trumpet* **(a)**; and given the subject $s' =\\, $*Cat Stevens*, the model will now predict $o' =\\, $*guiter* **(b)**. \n",
    "\n",
    "If the computation from $\\mathbf{s}$ to $\\mathbf{o}$ is well-approximated by $operator$ parameterized by $W_r$ and $b_r$ **(c)**, then $\\Delta{\\mathbf{s}}$ **(d)** should tell us the direction of change from $\\mathbf{s}$ to $\\mathbf{s}'$. Thus, $\\tilde{\\mathbf{s}}=\\mathbf{s}+\\Delta\\mathbf{s}$ would be an approximation of $\\mathbf{s}'$ and patching $\\tilde{\\mathbf{s}}$ in place of $\\mathbf{s}$ should change the prediction to $o'$ = *guitar* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "53c632ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_delta_s(\n",
    "    operator, \n",
    "    source_subject, \n",
    "    target_subject,\n",
    "    rank = 100,\n",
    "    fix_latent_norm = None, # if set, will fix the norms of z_source and z_target\n",
    "):\n",
    "    w_p_inv = functional.low_rank_pinv(\n",
    "        matrix = operator.weight,\n",
    "        rank=rank,\n",
    "    )\n",
    "    hs_and_zs = functional.compute_hs_and_zs(\n",
    "        mt = mt,\n",
    "        prompt_template = operator.prompt_template,\n",
    "        subjects = [source_subject, target_subject],\n",
    "        h_layer= operator.h_layer,\n",
    "        z_layer=-1,\n",
    "    )\n",
    "\n",
    "    z_source = hs_and_zs.z_by_subj[source_subject]\n",
    "    z_target = hs_and_zs.z_by_subj[target_subject]\n",
    "    \n",
    "    z_source *= fix_latent_norm / z_source.norm() if fix_latent_norm is not None else 1.0\n",
    "    z_target *= z_source.norm() / z_target.norm() if fix_latent_norm is not None else 1.0\n",
    "\n",
    "    delta_s = w_p_inv @  (z_target.squeeze() - z_source.squeeze())\n",
    "\n",
    "    return delta_s, hs_and_zs\n",
    "\n",
    "delta_s, hs_and_zs = get_delta_s(\n",
    "    operator = operator,\n",
    "    source_subject = source.subject,\n",
    "    target_subject = target.subject,\n",
    "    rank = rank\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "ab1c7e88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(' men', 0.491),\n",
       " (' women', 0.476),\n",
       " (' girls', 0.004),\n",
       " (' both', 0.003),\n",
       " (' the', 0.002),\n",
       " (' boys', 0.002),\n",
       " ('\\n', 0.002),\n",
       " (' males', 0.002),\n",
       " (' people', 0.001),\n",
       " (' females', 0.001)]"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import baukit\n",
    "\n",
    "def get_intervention(h, int_layer, subj_idx):\n",
    "    def edit_output(output, layer):\n",
    "        if(layer != int_layer):\n",
    "            return output\n",
    "        functional.untuple(output)[:, subj_idx] = h \n",
    "        return output\n",
    "    return edit_output\n",
    "\n",
    "prompt = operator.prompt_template.format(source.subject)\n",
    "\n",
    "h_index, inputs = functional.find_subject_token_index(\n",
    "    mt=mt,\n",
    "    prompt=prompt,\n",
    "    subject=source.subject,\n",
    ")\n",
    "\n",
    "h_layer, z_layer = models.determine_layer_paths(model = mt, layers = [layer, -1])\n",
    "\n",
    "with baukit.TraceDict(\n",
    "    mt.model, layers = [h_layer, z_layer],\n",
    "    edit_output=get_intervention(\n",
    "#         h = hs_and_zs.h_by_subj[source.subject],         # let the computation proceed as usual\n",
    "        h = hs_and_zs.h_by_subj[source.subject] + delta_s, # replace s with s + delta_s\n",
    "        int_layer = h_layer, \n",
    "        subj_idx = h_index\n",
    "    )\n",
    ") as traces:\n",
    "    outputs = mt.model(\n",
    "        input_ids = inputs.input_ids,\n",
    "        attention_mask = inputs.attention_mask,\n",
    "    )\n",
    "\n",
    "lens.interpret_logits(\n",
    "    mt = mt, \n",
    "    logits = outputs.logits[0][-1], \n",
    "    get_proba=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c272c1",
   "metadata": {},
   "source": [
    "## Measuring causality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "51efa257",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.editors import LowRankPInvEditor\n",
    "\n",
    "svd = torch.svd(operator.weight.float())\n",
    "editor = LowRankPInvEditor(\n",
    "    lre=operator,\n",
    "    rank=rank,\n",
    "    svd=svd,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "88be35dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping adventurousness -> women | edit result= men (p=0.495) | success=(✗)\n",
      "Mapping aggressiveness -> women | edit result= women (p=0.565) | success=(✓)\n",
      "Mapping ambition -> women | edit result= women (p=0.696) | success=(✓)\n",
      "Mapping assertiveness -> women | edit result= women (p=0.605) | success=(✓)\n",
      "Mapping bravery -> women | edit result= women (p=0.671) | success=(✓)\n",
      "Mapping compassion -> men | edit result= men (p=0.867) | success=(✓)\n",
      "Mapping assertiveness -> women | edit result= women (p=0.605) | success=(✓)\n",
      "Mapping bravery -> women | edit result= women (p=0.671) | success=(✓)\n",
      "Mapping compassion -> men | edit result= men (p=0.867) | success=(✓)\n",
      "Mapping competitiveness -> women | edit result= women (p=0.705) | success=(✓)\n",
      "Mapping confidence -> women | edit result= women (p=0.794) | success=(✓)\n",
      "Mapping critical thinking -> women | edit result= men (p=0.516) | success=(✗)\n",
      "Mapping competitiveness -> women | edit result= women (p=0.705) | success=(✓)\n",
      "Mapping confidence -> women | edit result= women (p=0.794) | success=(✓)\n",
      "Mapping critical thinking -> women | edit result= men (p=0.516) | success=(✗)\n",
      "Mapping decisiveness -> women | edit result= women (p=0.764) | success=(✓)\n",
      "Mapping discipline -> women | edit result= women (p=0.898) | success=(✓)\n",
      "Mapping empathy -> men | edit result= men (p=0.854) | success=(✓)\n",
      "Mapping decisiveness -> women | edit result= women (p=0.764) | success=(✓)\n",
      "Mapping discipline -> women | edit result= women (p=0.898) | success=(✓)\n",
      "Mapping empathy -> men | edit result= men (p=0.854) | success=(✓)\n",
      "Mapping endurance -> women | edit result= women (p=0.703) | success=(✓)\n",
      "Mapping independence -> women | edit result= women (p=0.790) | success=(✓)\n",
      "Mapping intuition -> men | edit result= men (p=0.890) | success=(✓)\n",
      "Mapping endurance -> women | edit result= women (p=0.703) | success=(✓)\n",
      "Mapping independence -> women | edit result= women (p=0.790) | success=(✓)\n",
      "Mapping intuition -> men | edit result= men (p=0.890) | success=(✓)\n",
      "Mapping leadership -> women | edit result= women (p=0.798) | success=(✓)\n",
      "Mapping resilience -> women | edit result= women (p=0.536) | success=(✓)\n",
      "Mapping risk-taking -> women | edit result= women (p=0.497) | success=(✓)\n",
      "Mapping leadership -> women | edit result= women (p=0.798) | success=(✓)\n",
      "Mapping resilience -> women | edit result= women (p=0.536) | success=(✓)\n",
      "Mapping risk-taking -> women | edit result= women (p=0.497) | success=(✓)\n",
      "Mapping sensitivity -> men | edit result= men (p=0.865) | success=(✓)\n",
      "------------------------------------------------------------\n",
      "Causality (@1) = 0.8947368421052632\n",
      "------------------------------------------------------------\n",
      "Mapping sensitivity -> men | edit result= men (p=0.865) | success=(✓)\n",
      "------------------------------------------------------------\n",
      "Causality (@1) = 0.8947368421052632\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# precomputing latents to speed things up\n",
    "hs_and_zs = functional.compute_hs_and_zs(\n",
    "    mt = mt,\n",
    "    prompt_template = operator.prompt_template,\n",
    "    subjects = [sample.subject for sample in test.samples],\n",
    "    h_layer= operator.h_layer,\n",
    "    z_layer=-1,\n",
    "    batch_size = 2\n",
    ")\n",
    "\n",
    "success = 0\n",
    "fails = 0\n",
    "\n",
    "for sample in test.samples:\n",
    "    target = test_targets.get(sample)\n",
    "    assert target is not None\n",
    "    edit_result = editor(\n",
    "        subject = sample.subject,\n",
    "        target = target.subject\n",
    "    )\n",
    "    \n",
    "    success_flag = functional.is_nontrivial_prefix(\n",
    "        prediction=edit_result.predicted_tokens[0].token, target=target.object\n",
    "    )\n",
    "    \n",
    "    print(f\"Mapping {sample.subject} -> {target.object} | edit result={edit_result.predicted_tokens[0]} | success=({functional.get_tick_marker(success_flag)})\")\n",
    "    \n",
    "    success += success_flag\n",
    "    fails += not success_flag\n",
    "    \n",
    "causality = success / (success + fails)\n",
    "\n",
    "print(\"------------------------------------------------------------\")\n",
    "print(f\"Causality (@1) = {causality}\")\n",
    "print(\"------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c587ae85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d6d2a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
